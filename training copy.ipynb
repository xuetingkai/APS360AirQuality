{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the Weather and PM2.5 Data\n",
    "\n",
    "- using min-max normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preprocessing import load_X_y\n",
    "import pandas as pd\n",
    "\n",
    "weather_df, pollutant_df = load_X_y()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_dew_point_v</th>\n",
       "      <th>max_relative_humidity_v</th>\n",
       "      <th>max_temperature_v</th>\n",
       "      <th>max_wind_speed_v</th>\n",
       "      <th>min_dew_point_v</th>\n",
       "      <th>min_relative_humidity_v</th>\n",
       "      <th>min_temperature_v</th>\n",
       "      <th>min_wind_speed_v</th>\n",
       "      <th>precipitation_v</th>\n",
       "      <th>rain_v</th>\n",
       "      <th>snow_v</th>\n",
       "      <th>snow_on_ground_v</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7304.000000</td>\n",
       "      <td>7304.000000</td>\n",
       "      <td>7304.000000</td>\n",
       "      <td>7304.000000</td>\n",
       "      <td>7304.000000</td>\n",
       "      <td>7304.000000</td>\n",
       "      <td>7304.000000</td>\n",
       "      <td>7304.000000</td>\n",
       "      <td>7304.000000</td>\n",
       "      <td>7304.000000</td>\n",
       "      <td>7304.000000</td>\n",
       "      <td>7304.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.514953</td>\n",
       "      <td>0.571856</td>\n",
       "      <td>0.524171</td>\n",
       "      <td>0.571205</td>\n",
       "      <td>0.518990</td>\n",
       "      <td>0.486371</td>\n",
       "      <td>0.519177</td>\n",
       "      <td>0.510449</td>\n",
       "      <td>0.491174</td>\n",
       "      <td>0.509921</td>\n",
       "      <td>0.247726</td>\n",
       "      <td>0.177910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.335249</td>\n",
       "      <td>0.218783</td>\n",
       "      <td>0.335174</td>\n",
       "      <td>0.255798</td>\n",
       "      <td>0.326785</td>\n",
       "      <td>0.261585</td>\n",
       "      <td>0.314073</td>\n",
       "      <td>0.271372</td>\n",
       "      <td>0.181621</td>\n",
       "      <td>0.244698</td>\n",
       "      <td>0.315199</td>\n",
       "      <td>0.278788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.179659</td>\n",
       "      <td>0.412934</td>\n",
       "      <td>0.182859</td>\n",
       "      <td>0.357715</td>\n",
       "      <td>0.201610</td>\n",
       "      <td>0.258465</td>\n",
       "      <td>0.210211</td>\n",
       "      <td>0.271762</td>\n",
       "      <td>0.344340</td>\n",
       "      <td>0.237410</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.495475</td>\n",
       "      <td>0.597683</td>\n",
       "      <td>0.536310</td>\n",
       "      <td>0.632265</td>\n",
       "      <td>0.500370</td>\n",
       "      <td>0.435290</td>\n",
       "      <td>0.517430</td>\n",
       "      <td>0.509554</td>\n",
       "      <td>0.504717</td>\n",
       "      <td>0.593525</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.863534</td>\n",
       "      <td>0.743629</td>\n",
       "      <td>0.859094</td>\n",
       "      <td>0.795591</td>\n",
       "      <td>0.851591</td>\n",
       "      <td>0.731753</td>\n",
       "      <td>0.828169</td>\n",
       "      <td>0.762208</td>\n",
       "      <td>0.617925</td>\n",
       "      <td>0.701439</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.360465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       max_dew_point_v  max_relative_humidity_v  max_temperature_v  \\\n",
       "count      7304.000000              7304.000000        7304.000000   \n",
       "mean          0.514953                 0.571856           0.524171   \n",
       "std           0.335249                 0.218783           0.335174   \n",
       "min           0.000000                 0.000000           0.000000   \n",
       "25%           0.179659                 0.412934           0.182859   \n",
       "50%           0.495475                 0.597683           0.536310   \n",
       "75%           0.863534                 0.743629           0.859094   \n",
       "max           1.000000                 1.000000           1.000000   \n",
       "\n",
       "       max_wind_speed_v  min_dew_point_v  min_relative_humidity_v  \\\n",
       "count       7304.000000      7304.000000              7304.000000   \n",
       "mean           0.571205         0.518990                 0.486371   \n",
       "std            0.255798         0.326785                 0.261585   \n",
       "min            0.000000         0.000000                 0.000000   \n",
       "25%            0.357715         0.201610                 0.258465   \n",
       "50%            0.632265         0.500370                 0.435290   \n",
       "75%            0.795591         0.851591                 0.731753   \n",
       "max            1.000000         1.000000                 1.000000   \n",
       "\n",
       "       min_temperature_v  min_wind_speed_v  precipitation_v       rain_v  \\\n",
       "count        7304.000000       7304.000000      7304.000000  7304.000000   \n",
       "mean            0.519177          0.510449         0.491174     0.509921   \n",
       "std             0.314073          0.271372         0.181621     0.244698   \n",
       "min             0.000000          0.000000         0.000000     0.000000   \n",
       "25%             0.210211          0.271762         0.344340     0.237410   \n",
       "50%             0.517430          0.509554         0.504717     0.593525   \n",
       "75%             0.828169          0.762208         0.617925     0.701439   \n",
       "max             1.000000          1.000000         1.000000     1.000000   \n",
       "\n",
       "            snow_v  snow_on_ground_v  \n",
       "count  7304.000000       7304.000000  \n",
       "mean      0.247726          0.177910  \n",
       "std       0.315199          0.278788  \n",
       "min       0.000000          0.000000  \n",
       "25%       0.000000          0.000000  \n",
       "50%       0.016667          0.000000  \n",
       "75%       0.583333          0.360465  \n",
       "max       1.000000          1.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_df_normalized = (weather_df-weather_df.min())/(weather_df.max()-weather_df.min())\n",
    "weather_df_normalized.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P2.5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7304.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.435753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.374638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>58.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              P2.5\n",
       "count  7304.000000\n",
       "mean      7.435753\n",
       "std       5.374638\n",
       "min       0.000000\n",
       "25%       3.916667\n",
       "50%       6.166667\n",
       "75%       9.333333\n",
       "max      58.666667"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pollutant_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7304])\n",
      "torch.Size([7304, 12])\n"
     ]
    }
   ],
   "source": [
    "pollutant_tensor = torch.tensor(pollutant_df['P2.5'].values)\n",
    "weather_tensor = torch.tensor(weather_df_normalized[:].values)\n",
    "\n",
    "print(pollutant_tensor.shape)\n",
    "print(weather_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pollutant_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "- We'll train using the first 5000+14 days to train, 1000+14 days to validate, 1000+14 days to test\n",
    "- Ensure sequencing is maintained\n",
    "- Test data will be the latest data as the model is intended to use historical data to predict future data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a custom dataset in a sliding window manner\n",
    "class WeatherPollutantDataset(Dataset):\n",
    "    def __init__(self, weather: torch.Tensor, pollutant: torch.Tensor, window:int):\n",
    "        self.weather = weather\n",
    "        self.pollutant = pollutant\n",
    "        # assumes the data starts on the same day\n",
    "        # assumes their length is the same\n",
    "        assert len(self.weather) == len(self.pollutant)\n",
    "        self.window = window\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        \"\"\"\n",
    "        Using weather input on day i,i+1,...,i+window-1 to predict pollutant output on i+window\n",
    "        \"\"\"\n",
    "        weather_input = self.weather[index:index+self.window].permute(1,0)\n",
    "        pollutant_output = self.pollutant[index+1:index+self.window+1]\n",
    "        return weather_input, pollutant_output\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.weather) - self.window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = WeatherPollutantDataset(weather=weather_tensor[:5014], pollutant=pollutant_tensor[:5014], window=14)\n",
    "val_set   = WeatherPollutantDataset(weather=weather_tensor[5014:6028], pollutant=pollutant_tensor[5014:6028], window=14)\n",
    "test_set  = WeatherPollutantDataset(weather=weather_tensor[6028:7042], pollutant=pollutant_tensor[6028:7042], window=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training data: 5000\n",
      "Number of validation data: 1000\n",
      "Number of testing data: 1000\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training data: %d\" % len(train_set))\n",
    "print(\"Number of validation data: %d\" % len(val_set))\n",
    "print(\"Number of testing data: %d\" % len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 12, 14])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader = DataLoader(test_set, batch_size=64)\n",
    "next(iter(test_loader))[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLSTMParam(nn.Module):\n",
    "    def __init__(self, in1, out1, out2, out3, hidden_size_lstm, num_layers_lstm, dense_neurons):\n",
    "        super().__init__()\n",
    "        self.name = \"CNNLSTM\"\n",
    "        self.out3 = out3\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.conv1 = nn.Conv1d(in_channels=12, out_channels=out1, kernel_size=3, stride=1, padding=1)\n",
    "   \n",
    "        self.norm1 = nn.BatchNorm1d(num_features = out1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=out1, out_channels=out2, kernel_size=3, stride=1, padding=1)\n",
    "        self.norm2 = nn.BatchNorm1d(num_features = out2)\n",
    "        self.conv3 = nn.Conv1d(in_channels=out2, out_channels=out3, kernel_size=3, stride=1, padding=1)\n",
    "        self.norm3 = nn.BatchNorm1d(num_features = out3)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=7*out3, hidden_size=hidden_size_lstm, num_layers=num_layers_lstm, batch_first=True)\n",
    "        self.linear1 = nn.Linear(hidden_size_lstm, dense_neurons)\n",
    "        self.linear2 = nn.Linear(dense_neurons, 14)\n",
    "\n",
    "    def forward(self, x):\n",
    "     \n",
    "        x = F.relu(self.norm1(self.conv1(x)))\n",
    "        x = F.relu(self.norm2(self.conv2(x)))\n",
    "        x = F.relu(self.norm3(self.conv3(x)))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 7*self.out3)\n",
    "        # x = x.squeeze(dim=2)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def evaluate(net, loader, criterion):\n",
    "    total_loss = 0.0\n",
    "    total_epoch = 0\n",
    "    for i, data in enumerate(loader, 0):\n",
    "        inputs, labels = data\n",
    "        # if torch.cuda.is_available():\n",
    "        inputs = inputs.float()\n",
    "        labels = labels.float()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        total_epoch += len(labels)\n",
    "    loss = float(total_loss) / (i + 1)\n",
    "    return loss\n",
    "\n",
    "def train_net(hyperparameters):\n",
    "    torch.manual_seed(1000)\n",
    "    \n",
    "    batch_size, learning_rate, num_epochs, in1, out1, out2, out3, hidden_size_lstm, num_layers_lstm, dense_neurons = hyperparameters\n",
    "    \n",
    "    net = CNNLSTMParam(in1, out1, out2, out3, hidden_size_lstm, num_layers_lstm, dense_neurons) \n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size = batch_size, shuffle=False)\n",
    "    val_loader = torch.utils.data.DataLoader(val_set, batch_size = batch_size, shuffle=False)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_loss = np.zeros(num_epochs)\n",
    "    val_loss = np.zeros(num_epochs)\n",
    "    \n",
    "    \n",
    "    \n",
    "    net.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_train_loss = 0.0\n",
    "        total_epoch = 0\n",
    "\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data\n",
    "            # if torch.torch.cuda.is_available():\n",
    "            inputs = inputs.float()\n",
    "            labels = labels.float()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs).squeeze()\n",
    "            labels = labels.squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Calculate the statistics\n",
    "            total_train_loss += loss.item()\n",
    "            # total_epoch += len(labels)\n",
    "            total_epoch += 1\n",
    "        train_loss[epoch] = float(total_train_loss) / (i+1)\n",
    "        val_loss[epoch] = evaluate(net, val_loader, criterion)\n",
    "        if epoch%10 == 0:\n",
    "            print((\"Epoch {}: Train loss: {} | Validation loss: {}\").format(epoch + 1,train_loss[epoch],val_loss[epoch]))\n",
    "        # Save the current model (checkpoint) to a file\n",
    "        # torch.save(net.state_dict(), 'ckpt/01112023_mse_batchsize_%d_learningrate_%d_epoch_%d'%(batch_size, learning_rate, 60+epoch))\n",
    "    net.eval()\n",
    "    print('Finished Training')\n",
    "    \n",
    "    with open('ga_runs.txt', 'a') as f:\n",
    "        print('Writing to file')\n",
    "        f.write(str(hyperparameters))\n",
    "        f.write('\\t')\n",
    "        f.write(str(train_loss[-1]))\n",
    "        f.write('\\n')\n",
    "    \n",
    "    return train_loss[-1]\n",
    "    # Write the train/test loss/err into CSV file for plotting later\n",
    "    # epochs = np.arange(1, num_epochs + 1)\n",
    "    # plt.title(\"Train loss\")\n",
    "    # plt.plot(epochs, train_loss)\n",
    "    # plt.xlabel(\"Epoch\")\n",
    "    # plt.ylabel(\"Loss\")\n",
    "\n",
    "    # plt.title(\"Validation loss\")\n",
    "    # plt.plot(epochs, val_loss)\n",
    "    # plt.xlabel(\"Epoch\")\n",
    "    # plt.ylabel(\"Loss\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyperparameters: [15, 0.005270588235294118, 14, 11, 29, 16, 49, 9, 3, 23]\n",
      "Epoch 1: Train loss: 37.92380979675019 | Validation loss: 35.641163523517434\n",
      "Epoch 11: Train loss: 21.396405211822714 | Validation loss: 35.321358588204454\n",
      "Finished Training\n",
      "Writing to file\n",
      "hyperparameters: [59, 0.009541176470588237, 15, 30, 115, 104, 108, 4, 3, 14]\n",
      "Epoch 1: Train loss: 50.154355610118195 | Validation loss: 35.01981544494629\n",
      "Epoch 11: Train loss: 21.68052267747767 | Validation loss: 34.192618762745575\n",
      "Finished Training\n",
      "Writing to file\n",
      "hyperparameters: [46, 0.0743764705882353, 30, 10, 123, 10, 117, 8, 3, 18]\n",
      "Epoch 1: Train loss: 25.33277659459945 | Validation loss: 36.62530582601374\n",
      "Epoch 11: Train loss: 21.676703067000854 | Validation loss: 36.22973864728754\n",
      "Epoch 21: Train loss: 21.67091398282882 | Validation loss: 36.30839816006747\n",
      "Finished Training\n",
      "Writing to file\n",
      "hyperparameters: [38, 0.04642352941176471, 11, 30, 123, 67, 121, 8, 2, 24]\n",
      "Epoch 1: Train loss: 25.84931685888406 | Validation loss: 37.5337034508034\n",
      "Epoch 11: Train loss: 21.394624319040414 | Validation loss: 36.156595318405714\n",
      "Finished Training\n",
      "Writing to file\n",
      "hyperparameters: [59, 0.09650588235294118, 57, 22, 60, 56, 79, 7, 3, 49]\n",
      "Epoch 1: Train loss: 26.44325143028708 | Validation loss: 36.91264629364014\n",
      "Epoch 11: Train loss: 22.048071070278393 | Validation loss: 36.27912740146412\n",
      "Epoch 21: Train loss: 21.792030620574952 | Validation loss: 37.39562301074757\n",
      "Epoch 31: Train loss: 21.574403841355267 | Validation loss: 35.32913151909323\n",
      "Epoch 41: Train loss: 21.4470910913804 | Validation loss: 34.3943328296437\n",
      "Epoch 51: Train loss: 21.163487709269805 | Validation loss: 33.699990693260645\n",
      "Finished Training\n",
      "Writing to file\n",
      "hyperparameters: [38, 0.07670588235294118, 31, 23, 91, 115, 62, 11, 1, 46]\n",
      "Epoch 1: Train loss: 26.18794985250993 | Validation loss: 41.09403343553896\n",
      "Epoch 11: Train loss: 21.932909235809788 | Validation loss: 37.56780384205006\n",
      "Epoch 21: Train loss: 21.613952899069496 | Validation loss: 36.4037381278144\n",
      "Epoch 31: Train loss: 21.34468731916312 | Validation loss: 34.64343992869059\n",
      "Finished Training\n",
      "Writing to file\n",
      "hyperparameters: [51, 0.035164705882352945, 19, 5, 102, 18, 85, 3, 3, 31]\n",
      "Epoch 1: Train loss: 27.40223972243492 | Validation loss: 37.752376079559326\n",
      "Epoch 11: Train loss: 21.50667859809567 | Validation loss: 37.788768124580386\n",
      "Finished Training\n",
      "Writing to file\n",
      "hyperparameters: [4, 0.049529411764705884, 39, 9, 96, 106, 13, 13, 1, 22]\n",
      "Epoch 1: Train loss: 19.464054545389114 | Validation loss: 39.55267143726349\n",
      "Epoch 11: Train loss: 21.591135746324063 | Validation loss: 34.39066243743896\n",
      "Epoch 21: Train loss: 21.59111602677107 | Validation loss: 34.39066243743896\n",
      "Epoch 31: Train loss: 21.59111602677107 | Validation loss: 34.39066243743896\n",
      "Finished Training\n",
      "Writing to file\n",
      "hyperparameters: [37, 0.049529411764705884, 10, 19, 55, 120, 61, 7, 2, 20]\n",
      "Epoch 1: Train loss: 25.25816742812886 | Validation loss: 39.547083258628845\n",
      "Finished Training\n",
      "Writing to file\n",
      "hyperparameters: [36, 0.039047058823529414, 64, 27, 95, 19, 107, 4, 4, 45]\n",
      "Epoch 1: Train loss: 26.08576598013048 | Validation loss: 36.250331333705354\n",
      "Epoch 11: Train loss: 21.463480863639777 | Validation loss: 35.58256312779018\n",
      "Epoch 21: Train loss: 21.58584774998452 | Validation loss: 36.00707820483616\n",
      "Epoch 31: Train loss: 21.578194405535143 | Validation loss: 35.97828151498522\n",
      "Epoch 41: Train loss: 21.532324785808864 | Validation loss: 36.24928057193756\n",
      "Epoch 51: Train loss: 21.577512764244627 | Validation loss: 35.95394146442413\n",
      "Epoch 61: Train loss: 21.49753282653342 | Validation loss: 35.582669905253816\n",
      "Finished Training\n",
      "Writing to file\n",
      "hyperparameters: [7, 0.04487058823529412, 57, 27, 84, 97, 73, 2, 4, 31]\n",
      "Epoch 1: Train loss: 20.94403900263401 | Validation loss: 38.511388436897654\n",
      "Epoch 11: Train loss: 20.44077450001573 | Validation loss: 37.03287926753918\n",
      "Epoch 21: Train loss: 20.886717551196373 | Validation loss: 35.47436546945905\n",
      "Epoch 31: Train loss: 21.832804966921156 | Validation loss: 33.91204481858473\n",
      "Epoch 41: Train loss: 21.75078200022918 | Validation loss: 33.910502128667765\n",
      "Epoch 51: Train loss: 21.75078200022918 | Validation loss: 33.910502128667765\n",
      "Finished Training\n",
      "Writing to file\n",
      "hyperparameters: [25, 0.08602352941176471, 16, 18, 61, 21, 24, 3, 2, 35]\n",
      "Epoch 1: Train loss: 23.538421633578835 | Validation loss: 37.008620363473895\n",
      "Epoch 11: Train loss: 21.472052604854106 | Validation loss: 36.06312950849533\n",
      "Finished Training\n",
      "Writing to file\n",
      "hyperparameters: [48, 0.09650588235294118, 63, 1, 105, 118, 109, 2, 4, 31]\n",
      "Epoch 1: Train loss: 24.572601095835367 | Validation loss: 38.31945491972424\n",
      "Epoch 11: Train loss: 21.933661905924478 | Validation loss: 37.496727307637535\n",
      "Epoch 21: Train loss: 21.803813298543293 | Validation loss: 36.956041108994256\n",
      "Epoch 31: Train loss: 21.690846738361177 | Validation loss: 36.426718848092214\n",
      "Epoch 41: Train loss: 21.611131470544 | Validation loss: 35.73996630169096\n",
      "Epoch 51: Train loss: 21.619309927168345 | Validation loss: 34.86195423489525\n",
      "Epoch 61: Train loss: 21.754179452714467 | Validation loss: 34.15416181655157\n",
      "Finished Training\n",
      "Writing to file\n",
      "hyperparameters: [8, 0.01691764705882353, 58, 21, 108, 20, 114, 13, 1, 29]\n",
      "Epoch 1: Train loss: 21.88784750697613 | Validation loss: 37.816598316192625\n",
      "Epoch 11: Train loss: 20.48458991661072 | Validation loss: 36.83600731277466\n",
      "Epoch 21: Train loss: 20.5946875872612 | Validation loss: 36.13114735984802\n",
      "Epoch 31: Train loss: 20.665565074300766 | Validation loss: 36.38507816314697\n",
      "Epoch 41: Train loss: 20.57093909230232 | Validation loss: 36.33760841178894\n",
      "Epoch 51: Train loss: 20.781199390363692 | Validation loss: 35.57888234519959\n",
      "Finished Training\n",
      "Writing to file\n",
      "hyperparameters: [38, 0.0017764705882352943, 14, 4, 59, 71, 6, 3, 3, 48]\n",
      "Epoch 1: Train loss: 69.97277977249838 | Validation loss: 67.96938200350161\n",
      "Epoch 11: Train loss: 21.878885086512927 | Validation loss: 34.577543876789235\n",
      "Finished Training\n",
      "Writing to file\n",
      "hyperparameters: [31, 0.09495294117647059, 58, 27, 45, 109, 76, 10, 3, 21]\n",
      "Epoch 1: Train loss: 25.883048205831905 | Validation loss: 38.49167042067557\n",
      "Epoch 11: Train loss: 21.562074885692127 | Validation loss: 37.44513142470157\n",
      "Epoch 21: Train loss: 21.473525866314215 | Validation loss: 36.66396747935902\n",
      "Epoch 31: Train loss: 21.65026855542336 | Validation loss: 35.22705750031905\n",
      "Epoch 41: Train loss: 21.8195153625659 | Validation loss: 34.396940578113906\n",
      "Epoch 51: Train loss: 21.586814927098192 | Validation loss: 34.64027224165021\n",
      "Finished Training\n",
      "Writing to file\n",
      "hyperparameters: [19, 0.05962352941176471, 46, 20, 104, 20, 58, 11, 4, 23]\n",
      "Epoch 1: Train loss: 23.676501840353012 | Validation loss: 38.26258484372553\n",
      "Epoch 11: Train loss: 21.15721515543533 | Validation loss: 37.09586331979284\n",
      "Epoch 21: Train loss: 21.27497094982501 | Validation loss: 36.43208350775377\n",
      "Epoch 31: Train loss: 21.35210632352215 | Validation loss: 35.91717337662319\n",
      "Epoch 41: Train loss: 21.431525765269093 | Validation loss: 34.87684616502726\n",
      "Finished Training\n",
      "Writing to file\n",
      "hyperparameters: [36, 0.0685529411764706, 30, 25, 26, 30, 7, 10, 4, 37]\n",
      "Epoch 1: Train loss: 26.04068651130731 | Validation loss: 36.788360595703125\n",
      "Epoch 11: Train loss: 21.616022405864523 | Validation loss: 36.19976993969509\n",
      "Epoch 21: Train loss: 21.637348565266286 | Validation loss: 36.20158181871687\n",
      "Finished Training\n",
      "Writing to file\n",
      "hyperparameters: [27, 0.06467058823529412, 47, 22, 64, 115, 49, 12, 1, 33]\n",
      "Epoch 1: Train loss: 25.06728686953104 | Validation loss: 40.95179639991961\n",
      "Epoch 11: Train loss: 21.516151974278113 | Validation loss: 38.24213415070584\n",
      "Epoch 21: Train loss: 21.015571572447335 | Validation loss: 35.239810617346514\n",
      "Epoch 31: Train loss: 20.696561115884013 | Validation loss: 34.66927854010933\n",
      "Epoch 41: Train loss: 20.48577516277631 | Validation loss: 34.75929124731766\n",
      "Finished Training\n",
      "Writing to file\n",
      "hyperparameters: [19, 0.04991764705882353, 53, 14, 24, 74, 86, 11, 4, 28]\n",
      "Epoch 1: Train loss: 23.75703821737658 | Validation loss: 37.69803672016791\n",
      "Epoch 11: Train loss: 21.25995772580306 | Validation loss: 36.97028392665791\n",
      "Epoch 21: Train loss: 21.180582090083398 | Validation loss: 38.064698264283955\n",
      "Epoch 31: Train loss: 21.322582162690885 | Validation loss: 36.252072748148215\n",
      "Epoch 41: Train loss: 21.35432515090162 | Validation loss: 35.85806968976866\n",
      "Epoch 51: Train loss: 21.3987117247148 | Validation loss: 35.282471890719435\n",
      "Finished Training\n",
      "Writing to file\n",
      "gen\tnevals\n",
      "0  \t20    \n",
      "hyperparameters: [38, 0.04642352941176471, 58, 21, 108, 20, 121, 8, 2, 24]\n",
      "Epoch 1: Train loss: 26.486160216909468 | Validation loss: 38.18415964974297\n",
      "Epoch 11: Train loss: 21.201168199380238 | Validation loss: 37.9589647187127\n",
      "Epoch 21: Train loss: 21.085184997229867 | Validation loss: 35.26821026095637\n",
      "Epoch 31: Train loss: 20.821746916030392 | Validation loss: 35.82900652179011\n",
      "Epoch 41: Train loss: 20.92347987131639 | Validation loss: 34.99008586671617\n",
      "Epoch 51: Train loss: 20.17759006831682 | Validation loss: 34.95589044358995\n",
      "Finished Training\n",
      "Writing to file\n",
      "hyperparameters: [8, 0.01691764705882353, 11, 30, 123, 67, 114, 13, 1, 29]\n",
      "Epoch 1: Train loss: 21.365585283756257 | Validation loss: 37.72810129737854\n",
      "Epoch 11: Train loss: 20.35476602830887 | Validation loss: 37.122908527374264\n",
      "Finished Training\n",
      "Writing to file\n",
      "hyperparameters: [27, 0.06467058823529412, 47, 22, 64, 115, 49, 12, 1, 33]\n",
      "Epoch 1: Train loss: 25.06728686953104 | Validation loss: 40.95179639991961\n",
      "Epoch 11: Train loss: 21.516151974278113 | Validation loss: 38.24213415070584\n",
      "Epoch 21: Train loss: 21.015571572447335 | Validation loss: 35.239810617346514\n",
      "Epoch 31: Train loss: 20.696561115884013 | Validation loss: 34.66927854010933\n",
      "Epoch 41: Train loss: 20.48577516277631 | Validation loss: 34.75929124731766\n",
      "Finished Training\n",
      "Writing to file\n",
      "hyperparameters: [27, 0.06467058823529412, 47, 22, 64, 115, 49, 12, 1, 33]\n",
      "Epoch 1: Train loss: 25.06728686953104 | Validation loss: 40.95179639991961\n",
      "Epoch 11: Train loss: 21.516151974278113 | Validation loss: 38.24213415070584\n",
      "Epoch 21: Train loss: 21.015571572447335 | Validation loss: 35.239810617346514\n",
      "Epoch 31: Train loss: 20.696561115884013 | Validation loss: 34.66927854010933\n",
      "Epoch 41: Train loss: 20.48577516277631 | Validation loss: 34.75929124731766\n",
      "Finished Training\n",
      "Writing to file\n",
      "hyperparameters: [27, 0.06467058823529412, 47, 22, 64, 115, 49, 12, 1, 33]\n",
      "Epoch 1: Train loss: 25.06728686953104 | Validation loss: 40.95179639991961\n",
      "Epoch 11: Train loss: 21.516151974278113 | Validation loss: 38.24213415070584\n",
      "Epoch 21: Train loss: 21.015571572447335 | Validation loss: 35.239810617346514\n",
      "Epoch 31: Train loss: 20.696561115884013 | Validation loss: 34.66927854010933\n",
      "Epoch 41: Train loss: 20.48577516277631 | Validation loss: 34.75929124731766\n",
      "Finished Training\n",
      "Writing to file\n",
      "hyperparameters: [27, 0.06467058823529412, 47, 22, 64, 115, 49, 12, 1, 33]\n",
      "Epoch 1: Train loss: 25.06728686953104 | Validation loss: 40.95179639991961\n",
      "Epoch 11: Train loss: 21.516151974278113 | Validation loss: 38.24213415070584\n",
      "Epoch 21: Train loss: 21.015571572447335 | Validation loss: 35.239810617346514\n",
      "Epoch 31: Train loss: 20.696561115884013 | Validation loss: 34.66927854010933\n",
      "Epoch 41: Train loss: 20.48577516277631 | Validation loss: 34.75929124731766\n",
      "Finished Training\n",
      "Writing to file\n",
      "hyperparameters: [7, 0.05147058823529412, 58, 7, 9, 32, 24, 8, 3, 31]\n",
      "Epoch 1: Train loss: 21.115287631638086 | Validation loss: 38.67267326208261\n",
      "Epoch 11: Train loss: 20.497033359641794 | Validation loss: 36.596108755031665\n",
      "Epoch 21: Train loss: 21.337284091594334 | Validation loss: 34.30723100608879\n",
      "Epoch 31: Train loss: 21.726168096503176 | Validation loss: 33.98662488443868\n",
      "Epoch 41: Train loss: 21.726168096503176 | Validation loss: 33.98662488443868\n",
      "Epoch 51: Train loss: 21.726168096503176 | Validation loss: 33.98662488443868\n",
      "Finished Training\n",
      "Writing to file\n",
      "hyperparameters: [38, 0.04642352941176471, 10, 19, 57, 67, 121, 8, 2, 24]\n",
      "Epoch 1: Train loss: 26.507706246592782 | Validation loss: 37.70824651364927\n",
      "Finished Training\n",
      "Writing to file\n",
      "hyperparameters: [37, 0.049529411764705884, 11, 30, 121, 120, 61, 7, 2, 20]\n",
      "Epoch 1: Train loss: 25.599379714797525 | Validation loss: 40.31372812816075\n",
      "Epoch 11: Train loss: 21.36171328144915 | Validation loss: 37.66938597815378\n",
      "Finished Training\n",
      "Writing to file\n",
      "hyperparameters: [36, 0.039047058823529414, 67, 11, 29, 16, 49, 4, 4, 45]\n",
      "Epoch 1: Train loss: 25.154355432489794 | Validation loss: 36.24581444263458\n",
      "Epoch 11: Train loss: 21.589541965251346 | Validation loss: 36.0302814074925\n",
      "Epoch 21: Train loss: 21.585756132928587 | Validation loss: 36.012521096638274\n",
      "Epoch 31: Train loss: 21.580828742157642 | Validation loss: 35.99771119867052\n",
      "Epoch 41: Train loss: 21.597395262272236 | Validation loss: 35.98780012130737\n",
      "Epoch 51: Train loss: 21.56182082008115 | Validation loss: 35.942371010780334\n",
      "Epoch 61: Train loss: 21.573018642638228 | Validation loss: 35.96900345597948\n",
      "Finished Training\n",
      "Writing to file\n",
      "hyperparameters: [15, 0.005270588235294118, 11, 27, 95, 19, 107, 9, 3, 23]\n",
      "Epoch 1: Train loss: 38.55118743816536 | Validation loss: 35.229545568352314\n",
      "Epoch 11: Train loss: 21.408313211834358 | Validation loss: 35.16499936402734\n",
      "Finished Training\n",
      "Writing to file\n",
      "1  \t11    \n",
      "hyperparameters: [8, 0.01691764705882353, 58, 21, 108, 20, 114, 13, 1, 29]\n",
      "Epoch 1: Train loss: 21.88784750697613 | Validation loss: 37.816598316192625\n",
      "Epoch 11: Train loss: 20.48458991661072 | Validation loss: 36.83600731277466\n",
      "Epoch 21: Train loss: 20.5946875872612 | Validation loss: 36.13114735984802\n",
      "Epoch 31: Train loss: 20.665565074300766 | Validation loss: 36.38507816314697\n",
      "Epoch 41: Train loss: 20.57093909230232 | Validation loss: 36.33760841178894\n",
      "Epoch 51: Train loss: 20.781199390363692 | Validation loss: 35.57888234519959\n",
      "Finished Training\n",
      "Writing to file\n",
      "hyperparameters: [8, 0.01691764705882353, 58, 21, 108, 20, 114, 13, 1, 29]\n",
      "Epoch 1: Train loss: 21.88784750697613 | Validation loss: 37.816598316192625\n",
      "Epoch 11: Train loss: 20.48458991661072 | Validation loss: 36.83600731277466\n",
      "Epoch 21: Train loss: 20.5946875872612 | Validation loss: 36.13114735984802\n",
      "Epoch 31: Train loss: 20.665565074300766 | Validation loss: 36.38507816314697\n",
      "Epoch 41: Train loss: 20.57093909230232 | Validation loss: 36.33760841178894\n",
      "Epoch 51: Train loss: 20.781199390363692 | Validation loss: 35.57888234519959\n",
      "Finished Training\n",
      "Writing to file\n",
      "hyperparameters: [27, 0.06467058823529412, 47, 22, 64, 115, 49, 12, 1, 33]\n",
      "Epoch 1: Train loss: 25.06728686953104 | Validation loss: 40.95179639991961\n",
      "Epoch 11: Train loss: 21.516151974278113 | Validation loss: 38.24213415070584\n",
      "Epoch 21: Train loss: 21.015571572447335 | Validation loss: 35.239810617346514\n",
      "Epoch 31: Train loss: 20.696561115884013 | Validation loss: 34.66927854010933\n",
      "Epoch 41: Train loss: 20.48577516277631 | Validation loss: 34.75929124731766\n",
      "Finished Training\n",
      "Writing to file\n",
      "hyperparameters: [8, 0.01691764705882353, 11, 30, 123, 67, 114, 13, 1, 29]\n",
      "Epoch 1: Train loss: 21.365585283756257 | Validation loss: 37.72810129737854\n",
      "Epoch 11: Train loss: 20.35476602830887 | Validation loss: 37.122908527374264\n",
      "Finished Training\n",
      "Writing to file\n",
      "hyperparameters: [27, 0.06661176470588236, 65, 4, 64, 37, 60, 13, 2, 24]\n",
      "Epoch 1: Train loss: 24.37059175375328 | Validation loss: 40.15549760115774\n",
      "Epoch 11: Train loss: 21.40687939108059 | Validation loss: 38.35034502179999\n",
      "Epoch 21: Train loss: 21.30284995609714 | Validation loss: 37.05337553902676\n",
      "Epoch 31: Train loss: 20.840204184414237 | Validation loss: 37.970233722736964\n",
      "Epoch 41: Train loss: 21.266026445454166 | Validation loss: 35.86687117501309\n",
      "Epoch 51: Train loss: 21.150969908442548 | Validation loss: 35.69824911418714\n",
      "Epoch 61: Train loss: 21.377129516454154 | Validation loss: 35.1084423190669\n",
      "Finished Training\n",
      "Writing to file\n",
      "hyperparameters: [26, 0.09495294117647059, 37, 19, 65, 103, 102, 9, 4, 37]\n",
      "Epoch 1: Train loss: 23.390862779604955 | Validation loss: 38.63325416124784\n",
      "Epoch 11: Train loss: 21.441887442929758 | Validation loss: 37.19068754024995\n",
      "Epoch 21: Train loss: 21.44045974430025 | Validation loss: 36.035536668239494\n",
      "Epoch 31: Train loss: 21.62102957625772 | Validation loss: 34.402489686623596\n",
      "Finished Training\n",
      "Writing to file\n",
      "hyperparameters: [38, 0.04642352941176471, 58, 21, 108, 20, 121, 8, 2, 24]\n",
      "Epoch 1: Train loss: 26.486160216909468 | Validation loss: 38.18415964974297\n",
      "Epoch 11: Train loss: 21.201168199380238 | Validation loss: 37.9589647187127\n",
      "Epoch 21: Train loss: 21.085184997229867 | Validation loss: 35.26821026095637\n",
      "Epoch 31: Train loss: 20.821746916030392 | Validation loss: 35.82900652179011\n",
      "Epoch 41: Train loss: 20.92347987131639 | Validation loss: 34.99008586671617\n",
      "Epoch 51: Train loss: 20.17759006831682 | Validation loss: 34.95589044358995\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\gretk\\Documents\\APS360AirQuality\\training copy.ipynb Cell 18\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gretk/Documents/APS360AirQuality/training%20copy.ipynb#X23sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m toolbox\u001b[39m.\u001b[39mregister(\u001b[39m'\u001b[39m\u001b[39mevaluate\u001b[39m\u001b[39m'\u001b[39m, train_evaluate)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gretk/Documents/APS360AirQuality/training%20copy.ipynb#X23sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m population \u001b[39m=\u001b[39m toolbox\u001b[39m.\u001b[39mpopulation(n\u001b[39m=\u001b[39mpopulation_size)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/gretk/Documents/APS360AirQuality/training%20copy.ipynb#X23sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m r \u001b[39m=\u001b[39m algorithms\u001b[39m.\u001b[39;49meaSimple(population, toolbox, cxpb\u001b[39m=\u001b[39;49m\u001b[39m0.4\u001b[39;49m, mutpb\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m, ngen\u001b[39m=\u001b[39;49mnum_generations, verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gretk/Documents/APS360AirQuality/training%20copy.ipynb#X23sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m best_individual \u001b[39m=\u001b[39m tools\u001b[39m.\u001b[39mselBest(population, k\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gretk/Documents/APS360AirQuality/training%20copy.ipynb#X23sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mBest ever individual = \u001b[39m\u001b[39m'\u001b[39m, best_individual, \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mFitness = \u001b[39m\u001b[39m'\u001b[39m, best_individual\u001b[39m.\u001b[39mfitness\u001b[39m.\u001b[39mvalues[\u001b[39m0\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\deap\\algorithms.py:173\u001b[0m, in \u001b[0;36meaSimple\u001b[1;34m(population, toolbox, cxpb, mutpb, ngen, stats, halloffame, verbose)\u001b[0m\n\u001b[0;32m    171\u001b[0m invalid_ind \u001b[39m=\u001b[39m [ind \u001b[39mfor\u001b[39;00m ind \u001b[39min\u001b[39;00m offspring \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m ind\u001b[39m.\u001b[39mfitness\u001b[39m.\u001b[39mvalid]\n\u001b[0;32m    172\u001b[0m fitnesses \u001b[39m=\u001b[39m toolbox\u001b[39m.\u001b[39mmap(toolbox\u001b[39m.\u001b[39mevaluate, invalid_ind)\n\u001b[1;32m--> 173\u001b[0m \u001b[39mfor\u001b[39;00m ind, fit \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(invalid_ind, fitnesses):\n\u001b[0;32m    174\u001b[0m     ind\u001b[39m.\u001b[39mfitness\u001b[39m.\u001b[39mvalues \u001b[39m=\u001b[39m fit\n\u001b[0;32m    176\u001b[0m \u001b[39m# Update the hall of fame with the generated individuals\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\gretk\\Documents\\APS360AirQuality\\training copy.ipynb Cell 18\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gretk/Documents/APS360AirQuality/training%20copy.ipynb#X23sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39m#[26, 4, [13, 18, 7, 1], [3, 3, 3, 3], [1, 1, 1, 1], [1, 1, 1, 1], 4, 1, None, 0.04515, 1833, 100]\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gretk/Documents/APS360AirQuality/training%20copy.ipynb#X23sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhyperparameters: \u001b[39m\u001b[39m{\u001b[39;00mhyperparameters\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/gretk/Documents/APS360AirQuality/training%20copy.ipynb#X23sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m loss \u001b[39m=\u001b[39m train_net(hyperparameters)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gretk/Documents/APS360AirQuality/training%20copy.ipynb#X23sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39mreturn\u001b[39;00m [loss]\n",
      "\u001b[1;32mc:\\Users\\gretk\\Documents\\APS360AirQuality\\training copy.ipynb Cell 18\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gretk/Documents/APS360AirQuality/training%20copy.ipynb#X23sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gretk/Documents/APS360AirQuality/training%20copy.ipynb#X23sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/gretk/Documents/APS360AirQuality/training%20copy.ipynb#X23sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m outputs \u001b[39m=\u001b[39m net(inputs)\u001b[39m.\u001b[39msqueeze()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gretk/Documents/APS360AirQuality/training%20copy.ipynb#X23sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39msqueeze()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gretk/Documents/APS360AirQuality/training%20copy.ipynb#X23sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\gretk\\Documents\\APS360AirQuality\\training copy.ipynb Cell 18\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gretk/Documents/APS360AirQuality/training%20copy.ipynb#X23sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/gretk/Documents/APS360AirQuality/training%20copy.ipynb#X23sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm1(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gretk/Documents/APS360AirQuality/training%20copy.ipynb#X23sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x)))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gretk/Documents/APS360AirQuality/training%20copy.ipynb#X23sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm3(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv3(x)))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\batchnorm.py:151\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrack_running_stats:\n\u001b[0;32m    149\u001b[0m     \u001b[39m# TODO: if statement only here to tell the jit to skip emitting this when it is None\u001b[39;00m\n\u001b[0;32m    150\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_batches_tracked \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# type: ignore[has-type]\u001b[39;00m\n\u001b[1;32m--> 151\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_batches_tracked\u001b[39m.\u001b[39;49madd_(\u001b[39m1\u001b[39;49m)  \u001b[39m# type: ignore[has-type]\u001b[39;00m\n\u001b[0;32m    152\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmomentum \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# use cumulative moving average\u001b[39;00m\n\u001b[0;32m    153\u001b[0m             exponential_average_factor \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m \u001b[39m/\u001b[39m \u001b[39mfloat\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_batches_tracked)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from deap import base, creator, tools, algorithms\n",
    "from scipy.stats import bernoulli\n",
    "\n",
    "import numpy as np\n",
    "from bitstring import BitArray\n",
    "\n",
    "\n",
    "def train_evaluate(ga_individual_solution):\n",
    "    gene_length = 8\n",
    "    num_epochs = 30\n",
    "    \n",
    "\n",
    "    in1 = BitArray(ga_individual_solution[0:gene_length])\n",
    "    out1 = BitArray(ga_individual_solution[gene_length:2*gene_length])\n",
    "    out2 = BitArray(ga_individual_solution[2*gene_length:3*gene_length])\n",
    "    out3 = BitArray(ga_individual_solution[3*gene_length:4*gene_length])\n",
    "    hidden_size_lstm = BitArray(ga_individual_solution[4*gene_length:5*gene_length])\n",
    "    num_layers_lstm = BitArray(ga_individual_solution[5*gene_length:6*gene_length])\n",
    "    dense_neurons = BitArray(ga_individual_solution[6*gene_length:7*gene_length])\n",
    "    lr = BitArray(ga_individual_solution[7*gene_length:8*gene_length])\n",
    "    batch_size = BitArray(ga_individual_solution[8*gene_length:9*gene_length])\n",
    "    num_epochs = BitArray(ga_individual_solution[9*gene_length:10*gene_length])\n",
    "\n",
    "    in1 = in1.uint\n",
    "    out1 = out1.uint\n",
    "    out2 = out2.uint\n",
    "    out3 = out3.uint\n",
    "    hidden_size_lstm = hidden_size_lstm.uint\n",
    "    num_layers_lstm = num_layers_lstm.uint\n",
    "    dense_neurons = dense_neurons.uint\n",
    "  \n",
    "    lr = lr.uint\n",
    "    batch_size = batch_size.uint\n",
    "    num_epochs = num_epochs.uint\n",
    "\n",
    "    # resize hyperparameterss to be within range\n",
    "    in1 = int(np.interp(in1, [0, 255], [1, 31]))\n",
    "    out1 = int(np.interp(out1, [0, 255], [4, 128]))\n",
    "    out2 = int(np.interp(out2, [0, 255], [4, 128]))\n",
    "    out3 = int(np.interp(out3, [0, 255], [4, 128]))\n",
    "    hidden_size_lstm = int(np.interp(hidden_size_lstm, [0, 255], [2, 15]))\n",
    "    num_layers_lstm = int(np.interp(num_layers_lstm, [0, 255], [1, 5]))\n",
    "    dense_neurons = int(np.interp(dense_neurons, [0, 255], [10, 50]))\n",
    "    lr = np.interp(lr, [0, 255], [0.001, 0.1])\n",
    "    batch_size = int(np.interp(batch_size, [0, 255], [1, 64]))\n",
    "    num_epochs = int(np.interp(num_epochs, [0, 255], [10, 70]))\n",
    "        \n",
    "    #to optimise: seq, num_layers_conv, output_channels (num of kernels), hidden_size_lstm, num_layers_lstm, lr, batch_size, n_epoch\n",
    "    hyperparameters = [batch_size, lr, num_epochs, in1, out1, out2, out3, hidden_size_lstm, num_layers_lstm, dense_neurons]\n",
    "    #[26, 4, [13, 18, 7, 1], [3, 3, 3, 3], [1, 1, 1, 1], [1, 1, 1, 1], 4, 1, None, 0.04515, 1833, 100]\n",
    "    print(f'hyperparameters: {hyperparameters}')\n",
    "    loss = train_net(hyperparameters)\n",
    "    return [loss]\n",
    "\n",
    "\n",
    "population_size = 20\n",
    "num_generations = 5\n",
    "entire_bit_array_length = 11*8\n",
    "\n",
    "creator.create('FitnessMax', base.Fitness, weights=[-1.0])\n",
    "creator.create('Individual', list, fitness=creator.FitnessMax)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register('binary', bernoulli.rvs, 0.5)\n",
    "toolbox.register('individual', tools.initRepeat, creator.Individual, toolbox.binary, n=entire_bit_array_length)\n",
    "toolbox.register('population', tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "toolbox.register('mate', tools.cxOrdered)\n",
    "toolbox.register('mutate', tools.mutShuffleIndexes, indpb=0.6)\n",
    "toolbox.register('select', tools.selTournament, tournsize=int(population_size/2))\n",
    "toolbox.register('evaluate', train_evaluate)\n",
    "\n",
    "population = toolbox.population(n=population_size)\n",
    "r = algorithms.eaSimple(population, toolbox, cxpb=0.4, mutpb=0.1, ngen=num_generations, verbose=True)\n",
    "\n",
    "best_individual = tools.selBest(population, k=1)[0]\n",
    "print('Best ever individual = ', best_individual, '\\nFitness = ', best_individual.fitness.values[0])\n",
    "print(f'list of individuals = {best_individual}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss: 24.809430606437452 | Validation loss: 38.542580913614344\n"
     ]
    }
   ],
   "source": [
    "train_net([38, 0.04358588235294118, 36, 25, 98, 41, 32, 9, 1, 37])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
