{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the Weather and PM2.5 Data\n",
    "\n",
    "- using min-max normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preprocessing import load_X_y\n",
    "import pandas as pd\n",
    "\n",
    "weather_df, pollutant_df = load_X_y()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_dew_point_v</th>\n",
       "      <th>max_relative_humidity_v</th>\n",
       "      <th>max_temperature_v</th>\n",
       "      <th>max_wind_speed_v</th>\n",
       "      <th>min_dew_point_v</th>\n",
       "      <th>min_relative_humidity_v</th>\n",
       "      <th>min_temperature_v</th>\n",
       "      <th>min_wind_speed_v</th>\n",
       "      <th>precipitation_v</th>\n",
       "      <th>rain_v</th>\n",
       "      <th>snow_v</th>\n",
       "      <th>snow_on_ground_v</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7304.000000</td>\n",
       "      <td>7304.000000</td>\n",
       "      <td>7304.000000</td>\n",
       "      <td>7304.000000</td>\n",
       "      <td>7304.000000</td>\n",
       "      <td>7304.000000</td>\n",
       "      <td>7304.000000</td>\n",
       "      <td>7304.000000</td>\n",
       "      <td>7304.000000</td>\n",
       "      <td>7304.000000</td>\n",
       "      <td>7304.000000</td>\n",
       "      <td>7304.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.514953</td>\n",
       "      <td>0.571856</td>\n",
       "      <td>0.524171</td>\n",
       "      <td>0.571205</td>\n",
       "      <td>0.518990</td>\n",
       "      <td>0.486371</td>\n",
       "      <td>0.519177</td>\n",
       "      <td>0.510449</td>\n",
       "      <td>0.491174</td>\n",
       "      <td>0.509921</td>\n",
       "      <td>0.247726</td>\n",
       "      <td>0.177910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.335249</td>\n",
       "      <td>0.218783</td>\n",
       "      <td>0.335174</td>\n",
       "      <td>0.255798</td>\n",
       "      <td>0.326785</td>\n",
       "      <td>0.261585</td>\n",
       "      <td>0.314073</td>\n",
       "      <td>0.271372</td>\n",
       "      <td>0.181621</td>\n",
       "      <td>0.244698</td>\n",
       "      <td>0.315199</td>\n",
       "      <td>0.278788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.179659</td>\n",
       "      <td>0.412934</td>\n",
       "      <td>0.182859</td>\n",
       "      <td>0.357715</td>\n",
       "      <td>0.201610</td>\n",
       "      <td>0.258465</td>\n",
       "      <td>0.210211</td>\n",
       "      <td>0.271762</td>\n",
       "      <td>0.344340</td>\n",
       "      <td>0.237410</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.495475</td>\n",
       "      <td>0.597683</td>\n",
       "      <td>0.536310</td>\n",
       "      <td>0.632265</td>\n",
       "      <td>0.500370</td>\n",
       "      <td>0.435290</td>\n",
       "      <td>0.517430</td>\n",
       "      <td>0.509554</td>\n",
       "      <td>0.504717</td>\n",
       "      <td>0.593525</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.863534</td>\n",
       "      <td>0.743629</td>\n",
       "      <td>0.859094</td>\n",
       "      <td>0.795591</td>\n",
       "      <td>0.851591</td>\n",
       "      <td>0.731753</td>\n",
       "      <td>0.828169</td>\n",
       "      <td>0.762208</td>\n",
       "      <td>0.617925</td>\n",
       "      <td>0.701439</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.360465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       max_dew_point_v  max_relative_humidity_v  max_temperature_v  \\\n",
       "count      7304.000000              7304.000000        7304.000000   \n",
       "mean          0.514953                 0.571856           0.524171   \n",
       "std           0.335249                 0.218783           0.335174   \n",
       "min           0.000000                 0.000000           0.000000   \n",
       "25%           0.179659                 0.412934           0.182859   \n",
       "50%           0.495475                 0.597683           0.536310   \n",
       "75%           0.863534                 0.743629           0.859094   \n",
       "max           1.000000                 1.000000           1.000000   \n",
       "\n",
       "       max_wind_speed_v  min_dew_point_v  min_relative_humidity_v  \\\n",
       "count       7304.000000      7304.000000              7304.000000   \n",
       "mean           0.571205         0.518990                 0.486371   \n",
       "std            0.255798         0.326785                 0.261585   \n",
       "min            0.000000         0.000000                 0.000000   \n",
       "25%            0.357715         0.201610                 0.258465   \n",
       "50%            0.632265         0.500370                 0.435290   \n",
       "75%            0.795591         0.851591                 0.731753   \n",
       "max            1.000000         1.000000                 1.000000   \n",
       "\n",
       "       min_temperature_v  min_wind_speed_v  precipitation_v       rain_v  \\\n",
       "count        7304.000000       7304.000000      7304.000000  7304.000000   \n",
       "mean            0.519177          0.510449         0.491174     0.509921   \n",
       "std             0.314073          0.271372         0.181621     0.244698   \n",
       "min             0.000000          0.000000         0.000000     0.000000   \n",
       "25%             0.210211          0.271762         0.344340     0.237410   \n",
       "50%             0.517430          0.509554         0.504717     0.593525   \n",
       "75%             0.828169          0.762208         0.617925     0.701439   \n",
       "max             1.000000          1.000000         1.000000     1.000000   \n",
       "\n",
       "            snow_v  snow_on_ground_v  \n",
       "count  7304.000000       7304.000000  \n",
       "mean      0.247726          0.177910  \n",
       "std       0.315199          0.278788  \n",
       "min       0.000000          0.000000  \n",
       "25%       0.000000          0.000000  \n",
       "50%       0.016667          0.000000  \n",
       "75%       0.583333          0.360465  \n",
       "max       1.000000          1.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_df_normalized = (weather_df-weather_df.min())/(weather_df.max()-weather_df.min())\n",
    "weather_df_normalized.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P2.5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7304.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.435753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.374638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>58.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              P2.5\n",
       "count  7304.000000\n",
       "mean      7.435753\n",
       "std       5.374638\n",
       "min       0.000000\n",
       "25%       3.916667\n",
       "50%       6.166667\n",
       "75%       9.333333\n",
       "max      58.666667"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pollutant_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7304])\n",
      "torch.Size([7304, 12])\n"
     ]
    }
   ],
   "source": [
    "pollutant_tensor = torch.tensor(pollutant_df['P2.5'].values)\n",
    "weather_tensor = torch.tensor(weather_df_normalized[:].values)\n",
    "\n",
    "print(pollutant_tensor.shape)\n",
    "print(weather_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pollutant_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "- We'll train using the first 5000+14 days to train, 1000+14 days to validate, 1000+14 days to test\n",
    "- Ensure sequencing is maintained\n",
    "- Test data will be the latest data as the model is intended to use historical data to predict future data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a custom dataset in a sliding window manner\n",
    "class WeatherPollutantDataset(Dataset):\n",
    "    def __init__(self, weather: torch.Tensor, pollutant: torch.Tensor, window:int):\n",
    "        self.weather = weather\n",
    "        self.pollutant = pollutant\n",
    "        # assumes the data starts on the same day\n",
    "        # assumes their length is the same\n",
    "        assert len(self.weather) == len(self.pollutant)\n",
    "        self.window = window\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        \"\"\"\n",
    "        Using weather input on day i,i+1,...,i+window-1 to predict pollutant output on i+window\n",
    "        \"\"\"\n",
    "        weather_input = self.weather[index:index+self.window].permute(1,0)\n",
    "        pollutant_output = self.pollutant[index+1:index+self.window+1]\n",
    "        return weather_input, pollutant_output\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.weather) - self.window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = WeatherPollutantDataset(weather=weather_tensor[:5014], pollutant=pollutant_tensor[:5014], window=14)\n",
    "val_set   = WeatherPollutantDataset(weather=weather_tensor[5014:6028], pollutant=pollutant_tensor[5014:6028], window=14)\n",
    "test_set  = WeatherPollutantDataset(weather=weather_tensor[6028:7042], pollutant=pollutant_tensor[6028:7042], window=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training data: 5000\n",
      "Number of validation data: 1000\n",
      "Number of testing data: 1000\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training data: %d\" % len(train_set))\n",
    "print(\"Number of validation data: %d\" % len(val_set))\n",
    "print(\"Number of testing data: %d\" % len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 12, 14])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader = DataLoader(test_set, batch_size=64)\n",
    "next(iter(test_loader))[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLSTMParam(nn.Module):\n",
    "    def __init__(self, in1, out1, out2, out3, hidden_size_lstm, num_layers_lstm, dense_neurons):\n",
    "        super().__init__()\n",
    "        self.name = \"CNNLSTM\"\n",
    "        self.out3 = out3\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.conv1 = nn.Conv1d(in_channels=12, out_channels=out1, kernel_size=3, stride=1, padding=1)\n",
    "   \n",
    "        self.norm1 = nn.BatchNorm1d(num_features = out1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=out1, out_channels=out2, kernel_size=3, stride=1, padding=1)\n",
    "        self.norm2 = nn.BatchNorm1d(num_features = out2)\n",
    "        self.conv3 = nn.Conv1d(in_channels=out2, out_channels=out3, kernel_size=3, stride=1, padding=1)\n",
    "        self.norm3 = nn.BatchNorm1d(num_features = out3)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=7*out3, hidden_size=hidden_size_lstm, num_layers=num_layers_lstm, batch_first=True)\n",
    "        self.linear1 = nn.Linear(hidden_size_lstm, dense_neurons)\n",
    "        self.linear2 = nn.Linear(dense_neurons, 14)\n",
    "\n",
    "    def forward(self, x):\n",
    "     \n",
    "        x = F.relu(self.norm1(self.conv1(x)))\n",
    "        x = F.relu(self.norm2(self.conv2(x)))\n",
    "        x = F.relu(self.norm3(self.conv3(x)))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 7*self.out3)\n",
    "        # x = x.squeeze(dim=2)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def evaluate(net, loader, criterion):\n",
    "    total_loss = 0.0\n",
    "    total_epoch = 0\n",
    "    for i, data in enumerate(loader, 0):\n",
    "        inputs, labels = data\n",
    "        # if torch.cuda.is_available():\n",
    "        inputs = inputs.float()\n",
    "        labels = labels.float()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        total_epoch += len(labels)\n",
    "    loss = float(total_loss) / (i + 1)\n",
    "    return loss\n",
    "\n",
    "def train_net(hyperparameters):\n",
    "    torch.manual_seed(1000)\n",
    "    \n",
    "    batch_size, learning_rate, num_epochs, in1, out1, out2, out3, hidden_size_lstm, num_layers_lstm, dense_neurons = hyperparameters\n",
    "    \n",
    "    net = CNNLSTMParam(in1, out1, out2, out3, hidden_size_lstm, num_layers_lstm, dense_neurons) \n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size = batch_size, shuffle=False)\n",
    "    val_loader = torch.utils.data.DataLoader(val_set, batch_size = batch_size, shuffle=False)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_loss = np.zeros(num_epochs)\n",
    "    val_loss = np.zeros(num_epochs)\n",
    "    \n",
    "    \n",
    "    \n",
    "    net.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_train_loss = 0.0\n",
    "        total_epoch = 0\n",
    "\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data\n",
    "            # if torch.torch.cuda.is_available():\n",
    "            inputs = inputs.float()\n",
    "            labels = labels.float()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs).squeeze()\n",
    "            labels = labels.squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Calculate the statistics\n",
    "            total_train_loss += loss.item()\n",
    "            # total_epoch += len(labels)\n",
    "            total_epoch += 1\n",
    "        train_loss[epoch] = float(total_train_loss) / (i+1)\n",
    "        val_loss[epoch] = evaluate(net, val_loader, criterion)\n",
    "        if epoch%10 == 0:\n",
    "            print((\"Epoch {}: Train loss: {} | Validation loss: {}\").format(epoch,train_loss[epoch],val_loss[epoch]))\n",
    "        # Save the current model (checkpoint) to a file\n",
    "        # torch.save(net.state_dict(), 'ckpt/01112023_mse_batchsize_%d_learningrate_%d_epoch_%d'%(batch_size, learning_rate, 60+epoch))\n",
    "    net.eval()\n",
    "    print('Finished Training')\n",
    "    \n",
    "    with open('ga_runs.txt', 'a') as f:\n",
    "        print('Writing to file')\n",
    "        f.write(str(hyperparameters))\n",
    "        f.write('\\t')\n",
    "        f.write(str(train_loss[-1]))\n",
    "        f.write('\\n')\n",
    "    \n",
    "    return (train_loss[-1]+2*val_loss[-1])/3\n",
    "    # Write the train/test loss/err into CSV file for plotting later\n",
    "    # epochs = np.arange(1, num_epochs + 1)\n",
    "    # plt.title(\"Train loss\")\n",
    "    # plt.plot(epochs, train_loss)\n",
    "    # plt.xlabel(\"Epoch\")\n",
    "    # plt.ylabel(\"Loss\")\n",
    "\n",
    "    # plt.title(\"Validation loss\")\n",
    "    # plt.plot(epochs, val_loss)\n",
    "    # plt.xlabel(\"Epoch\")\n",
    "    # plt.ylabel(\"Loss\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyperparameters: [53, 0.025535294117647062, 66, 18, 51, 37, 81, 17, 1, 20]\n",
      "Epoch 0: Train loss: 26.361329093732333 | Validation loss: 36.73739430778905\n"
     ]
    }
   ],
   "source": [
    "from deap import base, creator, tools, algorithms\n",
    "from scipy.stats import bernoulli\n",
    "\n",
    "import numpy as np\n",
    "from bitstring import BitArray\n",
    "\n",
    "\n",
    "def train_evaluate(ga_individual_solution):\n",
    "    gene_length = 8\n",
    "    num_epochs = 30\n",
    "    \n",
    "\n",
    "    in1 = BitArray(ga_individual_solution[0:gene_length])\n",
    "    out1 = BitArray(ga_individual_solution[gene_length:2*gene_length])\n",
    "    out2 = BitArray(ga_individual_solution[2*gene_length:3*gene_length])\n",
    "    out3 = BitArray(ga_individual_solution[3*gene_length:4*gene_length])\n",
    "    hidden_size_lstm = BitArray(ga_individual_solution[4*gene_length:5*gene_length])\n",
    "    num_layers_lstm = BitArray(ga_individual_solution[5*gene_length:6*gene_length])\n",
    "    dense_neurons = BitArray(ga_individual_solution[6*gene_length:7*gene_length])\n",
    "    lr = BitArray(ga_individual_solution[7*gene_length:8*gene_length])\n",
    "    batch_size = BitArray(ga_individual_solution[8*gene_length:9*gene_length])\n",
    "    num_epochs = BitArray(ga_individual_solution[9*gene_length:10*gene_length])\n",
    "\n",
    "    in1 = in1.uint\n",
    "    out1 = out1.uint\n",
    "    out2 = out2.uint\n",
    "    out3 = out3.uint\n",
    "    hidden_size_lstm = hidden_size_lstm.uint\n",
    "    num_layers_lstm = num_layers_lstm.uint\n",
    "    dense_neurons = dense_neurons.uint\n",
    "  \n",
    "    lr = lr.uint\n",
    "    batch_size = batch_size.uint\n",
    "    num_epochs = num_epochs.uint\n",
    "\n",
    "    # resize hyperparameterss to be within range\n",
    "    in1 = int(np.interp(in1, [0, 255], [1, 31]))\n",
    "    out1 = int(np.interp(out1, [0, 255], [4, 100]))\n",
    "    out2 = int(np.interp(out2, [0, 255], [4, 100]))\n",
    "    out3 = int(np.interp(out3, [0, 255], [4, 100]))\n",
    "    hidden_size_lstm = int(np.interp(hidden_size_lstm, [0, 255], [2, 25]))\n",
    "    num_layers_lstm = int(np.interp(num_layers_lstm, [0, 255], [1, 5]))\n",
    "    dense_neurons = int(np.interp(dense_neurons, [0, 255], [10, 50]))\n",
    "    lr = np.interp(lr, [0, 255], [0.0005, 0.2])\n",
    "    batch_size = int(np.interp(batch_size, [0, 255], [1, 64]))\n",
    "    num_epochs = int(np.interp(num_epochs, [0, 255], [10, 100]))\n",
    "    num_layers_lstm = 1\n",
    "    #to optimise: seq, num_layers_conv, output_channels (num of kernels), hidden_size_lstm, num_layers_lstm, lr, batch_size, n_epoch\n",
    "    hyperparameters = [batch_size, lr, num_epochs, in1, out1, out2, out3, hidden_size_lstm, num_layers_lstm, dense_neurons]\n",
    "    #[26, 4, [13, 18, 7, 1], [3, 3, 3, 3], [1, 1, 1, 1], [1, 1, 1, 1], 4, 1, None, 0.04515, 1833, 100]\n",
    "    print(f'hyperparameters: {hyperparameters}')\n",
    "    loss = train_net(hyperparameters)\n",
    "    return [loss]\n",
    "\n",
    "\n",
    "population_size = 30\n",
    "num_generations = 5\n",
    "entire_bit_array_length = 11*8\n",
    "\n",
    "creator.create('FitnessMax', base.Fitness, weights=[-1.0])\n",
    "creator.create('Individual', list, fitness=creator.FitnessMax)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register('binary', bernoulli.rvs, 0.5)\n",
    "toolbox.register('individual', tools.initRepeat, creator.Individual, toolbox.binary, n=entire_bit_array_length)\n",
    "toolbox.register('population', tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "toolbox.register('mate', tools.cxOrdered)\n",
    "toolbox.register('mutate', tools.mutShuffleIndexes, indpb=0.6)\n",
    "toolbox.register('select', tools.selTournament, tournsize=int(population_size/2))\n",
    "toolbox.register('evaluate', train_evaluate)\n",
    "\n",
    "population = toolbox.population(n=population_size)\n",
    "r = algorithms.eaSimple(population, toolbox, cxpb=0.4, mutpb=0.1, ngen=num_generations, verbose=True)\n",
    "\n",
    "best_individual = tools.selBest(population, k=1)[0]\n",
    "print('Best ever individual = ', best_individual, '\\nFitness = ', best_individual.fitness.values[0])\n",
    "print(f'list of individuals = {best_individual}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss: 24.809430606437452 | Validation loss: 38.542580913614344\n",
      "Epoch 11: Train loss: 21.19750474799763 | Validation loss: 39.10666114312631\n",
      "Epoch 21: Train loss: 20.82503414470138 | Validation loss: 35.798597715519094\n",
      "Epoch 31: Train loss: 20.07201851497997 | Validation loss: 36.398271843239115\n",
      "Epoch 41: Train loss: 19.53788383621158 | Validation loss: 35.42676148591218\n",
      "Epoch 51: Train loss: 18.745859388149146 | Validation loss: 36.60471624798245\n",
      "Epoch 61: Train loss: 18.683589193857078 | Validation loss: 35.25573892946596\n",
      "Epoch 71: Train loss: 17.925649437037382 | Validation loss: 35.295965124059606\n",
      "Epoch 81: Train loss: 17.25579245704593 | Validation loss: 35.652593224136915\n",
      "Epoch 91: Train loss: 17.35503982775139 | Validation loss: 34.922996803566264\n",
      "Finished Training\n",
      "Writing to file\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15.671487145351641"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_net([38, 0.04358588235294118, 100, 25, 98, 41, 32, 9, 1, 37])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
